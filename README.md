# LLM-Powered Automation of ETL Pipelines and Documentation

This project demonstrates a framework for using Large Language Models (LLMs) to dynamically generate and document the "Transform" step in an ETL (Extract, Transform, Load) pipeline.

The core idea is to separate the static parts of the pipeline (extracting and loading data) from the dynamic part (transforming the data based on a user's request). An LLM is used to interpret a natural language request, generate the corresponding Python code for the transformation, and create documentation for it.

## Project Structure

The project is organized into the following directories and files:

```
.
├── etl/
│   ├── extract.py            # Reads raw data, merges it, and creates a clean CSV.
│   └── load.py               # Saves the final transformed data.
├── transforms/
│   └── dynamic_transforms.py # << LLM-generated transformation code goes here.
├── data/
│   ├── merged_data.csv       # Statically generated source of truth for transformations.
│   └── transformed_output.csv# The final output of the pipeline.
├── main_pipeline.py          # Main orchestrator script that runs the full pipeline.
├── Agent.md                  # Detailed schema + prompt template for the LLM.
├── user_prompt.txt           # << Where you write your data transformation request.
└── README.md                 # This file.
```

## Workflow

The workflow is designed to be simple and intuitive:

### Step 1: Write Your Transformation Request

Open the `user_prompt.txt` file and write what you want to do with the data in plain English.

**Example `user_prompt.txt`:**

```
Filter the dataset to include only records where the `WASTE_TYPE_TXT` is 'Bioabfall FFM' and the net weight (`NET_WEIGHT`) is greater than 1000 KG.
```

### Step 2: Generate the Transformation Code with an LLM

1.  Open `Agent.md` and locate the section "LLM Prompt Template for Dynamic ETL Transformation".
2.  Copy that prompt template and paste it into your preferred LLM interface (e.g., Gemini, ChatGPT).
3.  Replace the `{user_prompt}` placeholder in the template with the content of your `user_prompt.txt` file.
4.  The LLM will generate a Python function (`apply_transform`) and its documentation (`TRANSFORM_DOCUMENTATION`).

### Step 3: Update the Dynamic Transform Script

1.  Copy the entire code block generated by the LLM.
2.  Open `transforms/dynamic_transforms.py`.
3.  Delete the existing placeholder content and paste in the new, LLM-generated code.

### Step 4: Run the Pipeline

Execute the main pipeline script from your terminal:

```bash
python main_pipeline.py
```

The script will perform the following actions:

1.  **Extract**: If `data/merged_data.csv` doesn't exist, it runs the `etl/extract.py` script to create it from the raw `Daten` files.
2.  **Transform**: It loads `data/merged_data.csv` and applies the `apply_transform` function you just pasted into `transforms/dynamic_transforms.py`.
3.  **Document**: It prints the `TRANSFORM_DOCUMENTATION` that was generated by the LLM.
4.  **Load**: It saves the newly transformed data to `data/transformed_output.csv`.

You can now inspect the `transformed_output.csv` file to see the results of your dynamically generated transformation.
